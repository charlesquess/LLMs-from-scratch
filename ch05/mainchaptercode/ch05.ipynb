{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# 第5章：在无标签数据上预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.4\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.1+cu118\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- 在本章中，我们实现用于预训练LLM的训练循环和基本模型评估代码。\n",
    "- 在本章的最后，我们还将从OpenAI加载公开可用的预训练权重到我们的模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214765-7a73-42d5-95e9-302154b29db9",
   "metadata": {},
   "source": [
    "- 本章涵盖的主题如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "## 5.1 评估生成性文本模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- 本节我们从上一章的代码开始，简要回顾如何初始化一个GPT模型。\n",
    "- 然后，我们讨论LLM的基本评估指标。\n",
    "- 最后，在本节中，我们将这些评估指标应用于训练和验证数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "### 5.1.1 使用GPT生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- 我们使用上一章的代码初始化一个GPT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- 我们在上面使用了0.1的dropout，但现在通常在训练LLM时不使用dropout。\n",
    "- 现代LLM在`nn.Linear`层中也不使用查询、键和值矩阵的偏置向量（与早期的GPT模型不同），这是通过设置`\"qkv_bias\": False`来实现的。\n",
    "- 我们将上下文长度（`context_length`）从256个标记减少到减少计算资源需求，以便训练模型。而原始的1.24亿参数的GPT-2模型使用了1024个标记。 \n",
    "  - 这样可以使更多读者能够在笔记本电脑上跟随并执行代码示例。\n",
    "  - 然而，请随意将`context_length`增加到1024个标记（这不需要任何代码更改）。\n",
    "  - 我们稍后也会从预训练权重中加载一个`context_length`为1024的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- 接下来，我们使用上一章的 `generate_text_simple` 函数来生成文本。\n",
    "- 此外，我们定义了两个便捷函数，`text_to_token_ids`和`token_ids_to_text`，用于在本章中进行标记和文本表示之间的转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- 如上所示，模型尚未生成良好的文本，因为它还没有经过训练。\n",
    "- 我们如何以数值形式衡量或捕捉“良好的文本”，以便在训练过程中跟踪它？\n",
    "- 下一小节将介绍一些度量指标，用于计算生成输出的损失指标，我们可以通过这些指标来衡量训练进度。\n",
    "- 后续章节关于微调LLM的内容还将介绍其他衡量模型质量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
   },
   "source": [
    "### 5.1.2 计算文本生成损失：交叉熵和困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- 假设我们有一个`inputs`张量，其中包含2个训练样本（行）的标记ID。\n",
    "- 对应于`inputs`，`targets`包含我们希望模型生成的所需的标记ID。\n",
    "- 请注意，`targets`是`inputs`向右移动1个位置的结果，正如第2章实现数据加载器时所解释的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- 将`inputs`输入到模型中，我们获得包含2个输入样本（每个样本有3个标记）的logits向量。\n",
    "- 每个标记是一个50,257维的向量，对应于词汇表的大小。\n",
    "- 应用softmax函数，我们可以将logits张量转换为一个具有相同维度的概率分数张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- 下图中，为了说明目的使用了一个非常小的词汇表，展示了如何将概率分数转换回文本，这是我们上一章最后讨论的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- 如上一章所述，我们可以应用`argmax`函数将概率分数转换为预测的标记ID。\n",
    "- 上面的softmax函数为每个标记生成了一个50,257维的向量；`argmax`函数返回该向量中最高概率分数的位置，即给定标记的预测标记ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- 由于我们有2个输入批次，每个批次包含3个标记，因此我们获得2 by 3预测标记ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- 如果我们解码这些标记，我们会发现这些标记与我们希望模型预测的标记（即目标标记）有很大的不同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- 这是因为模型还没有经过训练。\n",
    "- 为了训练模型，我们需要知道模型预测的结果与正确的目标有多远。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- 对应于目标索引的标记概率如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- 我们希望最大化这些值，使其接近概率为1\n",
    "- 在数学优化中，最大化概率分数的对数比直接最大化概率分数更容易；这超出了本书的范围，但我在此处录制了一节讲座以更多细节： [L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- 接下来，我们计算平均对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- 目标是通过优化模型权重来使这个平均对数概率尽可能大。\n",
    "- 由于对数函数的存在，最大的可能值是0，而我们目前离0还很远。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- 在深度学习中，我们通过最小化负的平均对数概率值来代替最大化平均对数概率。在我们的情况下，深度学习中会最小化10.7722（即-10.7722的负值），使其接近0。\n",
    "- 负的-10.7722，即10.7722，在深度学习中也称为交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch已经实现了一个`cross_entropy`函数，该函数执行了前面的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- 在应用`cross_entropy`函数之前，我们先检查logits和targets的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- 对于PyTorch中的`cross_entropy`函数，我们希望将这些张量展平，通过在批次维度上进行组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- 请注意，目标是标记ID，这些标记ID也表示logits张量中我们希望最大化的索引位置。\n",
    "- PyTorch中的`cross_entropy`函数会自动在logits中对这些需要最大化的标记索引应用softmax和对数概率计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- 与交叉熵损失相关的一个概念是LLM的困惑度。\n",
    "- 困惑度仅仅是交叉熵损失的指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- 困惑度通常被认为更具可解释性，因为它可以理解为模型在每一步不确定的有效词汇表大小（在上面的例子中，这将是48,725个单词或标记）。\n",
    "- 换句话说，困惑度提供了一种衡量模型预测的概率分布与数据集中实际单词分布匹配程度的方法。\n",
    "- 类似于损失，较低的困惑度表示模型预测更接近实际分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "### 5.1.3 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- 我们使用一个相对较小的数据集来训练LLM（实际上只有一个短故事）。\n",
    "- 原因如下：\n",
    "  - 你可以在没有合适的GPU的情况下，仅使用笔记本电脑几分钟内运行代码示例。\n",
    "  - 训练过程相对较快（几分钟而不是几周），这对于教学目的非常好。\n",
    "  - 我们使用的是公共领域的文本，可以在不违反任何使用权或增加仓库大小的情况下包含在GitHub仓库中。\n",
    "\n",
    "- 例如，Llama 2 7B在2万亿个标记上训练需要184,320个A100 GPU小时。\n",
    "  - 在撰写本文时，AWS上8xA100云服务器的每小时费用约为30美元。\n",
    "  - 因此，通过粗略计算，训练这个LLM的费用约为184,320 / 8 * 30 = 690,000美元。\n",
    "\n",
    "- 下面，我们使用与第2章中相同的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "# 创建一个 SSLContext，可根据需要调整参数\n",
    "ctx = ssl.create_default_context()\n",
    "# 例如，禁用 session tickets（可选）\n",
    "# ctx.options |= ssl.OP_NO_TICKET\n",
    "\n",
    "def fetch_text(url, context):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, context=context) as resp:\n",
    "            return resp.read().decode('utf-8')\n",
    "    except ssl.SSLZeroReturnError:\n",
    "        # 对端优雅关闭，返回空内容或可重试\n",
    "        print(\"警告：SSL 连接已被对端关闭，返回空字符串。\")\n",
    "        return \"\"\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"网络错误：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    text_data = fetch_text(url, ctx)\n",
    "    # 如果你希望在遇到空内容时重试，可以在这里加个循环或递归\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- 通过打印前99个和后99个字符来快速检查文本是否加载正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 99 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 99 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- 由于只有5,145个标记，这段文本对于训练LLM来说非常短，但再次强调，这是为了教学目的（稍后我们还会加载预训练权重）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- 接下来，我们将数据集分为训练集和验证集，并使用第2章中的数据加载器为LLM训练准备批次。\n",
    "- 为了可视化目的，下图假设`max_length=6`，但对于训练加载器，我们将`max_length`设置为LLM支持的上下文长度。\n",
    "- 下图仅为了简化展示了输入标记。\n",
    "    - 由于我们训练LLM以预测文本中的下一个单词，目标标记与这些输入标记相同，只是目标标记向右移动了一位。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch02 import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- 我们使用相对较小的批次大小来减少计算资源的需求，并且因为数据集本身非常小。\n",
    "- 例如，Llama 2 7B使用了1024的批次大小进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- 一个可选的检查，确保数据加载正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- 另一个可选的检查，确保标记大小符合预期："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- 接下来，我们实现一个实用函数来计算给定批次的交叉熵损失。\n",
    "- 此外，我们实现第二个实用函数来计算数据加载器中指定数量批次的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- 如果你的机器上有一块支持CUDA的GPU，LLM将在GPU上进行训练，而无需对代码进行任何更改。\n",
    "- 通过`device`设置，我们确保数据加载到与LLM模型相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "## 5.2 训练LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- 在本节中，我们最终实现用于训练LLM的代码\n",
    "- 我们专注于一个简单的训练函数（如果你有兴趣使用更高级的技术来增强这个训练函数，例如学习率预热、余弦退火和梯度裁剪，请参阅 [Appendix D](../../appendix-D/01_main-chapter-code))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- 现在，让我们使用上面定义的训练函数来训练LLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.341\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.052\n",
      "Ep 2 (Step 000015): Train loss 6.048, Val loss 6.601\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.589, Val loss 6.480\n",
      "Ep 3 (Step 000025): Train loss 5.550, Val loss 6.413\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Ep 4 (Step 000030): Train loss 5.170, Val loss 6.370\n",
      "Ep 4 (Step 000035): Train loss 4.987, Val loss 6.377\n",
      "Every effort moves you a a him, and the of the picture to the picture. Gisburn, and a was, and the of the of the of the a. I had been. I had been the of the of the a of the. I had been\n",
      "Ep 5 (Step 000040): Train loss 4.327, Val loss 6.257\n",
      "Every effort moves you, I had been, I had been, I had been. Gisburn, I had been, I had been, in the of the of the of the of the of the of the honour of the of the man of the of the of\n",
      "Ep 6 (Step 000045): Train loss 4.064, Val loss 6.266\n",
      "Ep 6 (Step 000050): Train loss 3.585, Val loss 6.209\n",
      "Every effort moves you know the                                                \n",
      "Ep 7 (Step 000055): Train loss 3.622, Val loss 6.183\n",
      "Ep 7 (Step 000060): Train loss 2.821, Val loss 6.153\n",
      "Every effort moves you know the picture to see the picture.                    \"I he was his pictures-c.             \n",
      "Ep 8 (Step 000065): Train loss 2.390, Val loss 6.143\n",
      "Ep 8 (Step 000070): Train loss 2.032, Val loss 6.192\n",
      "Every effort moves you know,\" was, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you of the moment--as Jack himself, as he was his own of Jack's \"There were, in his\n",
      "Ep 9 (Step 000075): Train loss 1.665, Val loss 6.221\n",
      "Ep 9 (Step 000080): Train loss 1.318, Val loss 6.262\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on the last word.    \"!  \"Oh, I felt him back his head to the donkey--and I saw that, and down the room, I had\n",
      "Ep 10 (Step 000085): Train loss 1.022, Val loss 6.291\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"    \"I must he had the head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b86f0-b07d-40d7-b9d3-a9218917f204",
   "metadata": {},
   "source": [
    "- 请注意，你可能会在计算机上得到略有不同的损失值，但如果它们大致相似（训练损失低于1且验证损失低于7），则不必担心。\n",
    "- 小的差异可能是由于不同的GPU硬件、CUDA版本或新版本的PyTorch中的小变化引起的。\n",
    "- 即使你在CPU上运行示例，也可能观察到 slight 的差异；可能的原因是`nn.Dropout`在不同操作系统上的行为不同，具体取决于PyTorch的编译方式，如 [here on the PyTorch issue tracker](https://github.com/pytorch/pytorch/issues/121595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- 从上面的结果可以看出，模型开始时生成的是难以理解的字符串，而到了最后，它能够生成大致语法正确的句子。\n",
    "- 然而，根据训练集和验证集的损失值，我们可以看到模型开始过拟合。\n",
    "- 如果我们检查模型在最后生成的一些段落，会发现这些段落几乎完全verbatim地包含在训练集中——它只是简单地记住了训练数据。\n",
    "- 后面，我们将介绍一些解码策略，可以在一定程度上缓解这种记忆问题。\n",
    "- 请注意，这里的过拟合是因为我们使用了非常小的训练集，并且迭代了很多次。\n",
    "  - 这里的LLM训练主要用于教育目的；我们主要希望看到模型能够学会生成连贯的文本。\n",
    "  - 而不是花费数周或数月在昂贵的硬件上训练这个模型，我们稍后会加载预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**如果你有兴趣使用更高级的技术来增强这个训练函数，例如学习率预热、余弦退火和梯度裁剪，请参阅 [Appendix D](../../appendix-D/01_main-chapter-code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**如果你有兴趣使用更大的训练数据集和更长的训练时间，请参阅 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 解码策略以控制随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- 使用相对较小的LLM进行推理（如我们上面训练的GPT模型）相对便宜，因此即使你使用GPU进行训练，也不需要使用GPU进行推理。\n",
    "- 使用我们之前在简单训练函数中使用的`generate_text_simple`函数（来自上一章），我们可以逐个词（或标记）生成新文本。\n",
    "- 如第5.1.2节所述，下一个生成的标记是词汇表中概率分数最大的标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- 即使我们多次执行上面的`generate_text_simple`函数，LLM也将始终生成相同的输出。\n",
    "- 我们现在介绍两个概念，称为解码策略，来修改`generate_text_simple`：*温度缩放*和*top-k*采样。\n",
    "- 这些策略将允许模型控制生成文本的随机性和多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 温度缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- 之前，我们总是使用`torch.argmax`采样具有最高概率的标记作为下一个标记。\n",
    "- 为了增加多样性，我们可以使用`torch.multinomial(probs, num_samples=1)`从概率分布中采样下一个标记。\n",
    "- 在这里，每个索引被选中的机会对应于输入张量中该索引的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- 这里是一个简要回顾，说明如何生成下一个标记，假设一个非常小的词汇表以供说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- 与其通过`torch.argmax`确定最可能的标记，我们使用`torch.multinomial(probas, num_samples=1)`从softmax分布中采样来确定最可能的标记。\n",
    "- 为了说明目的，让我们看看使用原始softmax概率采样下一个标记1,000次会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- 我们可以通过一个称为温度缩放的概念来控制分布和选择过程。\n",
    "- “温度缩放”只是一个花哨的词，表示将logits除以一个大于0的数字。\n",
    "- 温度大于1会在应用softmax后导致标记概率分布更加均匀。\n",
    "- 温度小于1会在应用softmax后导致标记概率分布更加集中（更尖锐或更峰值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b5399",
   "metadata": {},
   "source": [
    "- 请注意，生成的dropout输出可能会因操作系统而异；你可以在PyTorch问题跟踪器上查看更多关于这种不一致的信息 [here on the PyTorch issue tracker](https://github.com/pytorch/pytorch/issues/121595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- 我们可以看到，通过温度0.1进行缩放后，分布变得更加尖锐，接近`torch.argmax`，从而使最可能的单词几乎总是被选中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- 通过温度5进行缩放后的概率分布更加均匀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- 假设LLM的输入是“every effort moves you”，使用上述方法有时会生成无意义的文本，例如“every effort moves you pizza”，这种情况有3.2%的概率（1000次中有32次）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- 为了能够使用更高的温度增加输出的多样性并减少生成无意义句子的概率，我们可以将采样的标记限制在最有可能的top-k个标记中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>\n",
    "\n",
    "- 请注意，该图中的数字已截断为小数点后两位，以减少视觉 clutter。Softmax 行中的值应加总为 1.0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- 在代码中，我们可以实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6fa49-6e99-459d-a517-d7d0f51c4f00",
   "metadata": {},
   "source": [
    "> 注意：  \n",
    ">\n",
    ">  之前代码单元的一个稍微更高效的实现如下：\n",
    ">\n",
    "> ```python\n",
    "> new_logits = torch.full_like( # create tensor containing -inf values\n",
    ">    next_token_logits, -torch.inf\n",
    ">)   \n",
    "> new_logits[top_pos] = next_token_logits[top_pos] # copy top k values into the -inf tensor\n",
    "> ```\n",
    "> <br>\n",
    "> 有关更多详细信息，请参阅 https://github.com/rasbt/LLMs-from-scratch/discussions/326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 修改文本生成函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- 上两小节介绍了温度采样和top-k采样。\n",
    "- 让我们使用这两个概念来修改之前用于通过LLM生成文本的`generate_simple`函数，创建一个新的`generate`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you in began to see it a a little to have been the by his knees\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 在PyTorch中加载和保存模型权重\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- 训练LLM非常计算密集，因此能够保存和加载LLM权重至关重要。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- 在PyTorch中，推荐的方式是通过`torch.save`函数保存模型权重，即所谓的`state_dict`，方法是调用`.state_dict()`方法并传递给`torch.save`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- 然后，我们可以将模型权重加载到一个新的`GPTModel`实例中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- 训练LLM时通常使用自适应优化器（如Adam或AdamW），而不是普通的SGD。\n",
    "- 这些自适应优化器为每个模型权重存储额外的参数，因此如果计划稍后继续预训练，保存这些优化器参数是有意义的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 从OpenAI加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- 之前，我们仅使用一个非常短的故事书来训练一个小的GPT-2模型，这是为了教学目的。\n",
    "- 对于感兴趣的读者，可以在[../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)中找到在完整Project Gutenberg图书语料库上的更长的预训练运行。\n",
    "- 幸运的是，我们不必花费数万到数十万美元来在大型预训练语料库上预训练模型，而是可以加载OpenAI提供的预训练权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "⚠️ 注意：某些用户在本节可能会遇到由于TensorFlow兼容性问题而导致的问题，特别是在某些Windows系统上。TensorFlow在这里仅用于加载原始的OpenAI GPT-2权重文件，然后将其转换为PyTorch格式。 如果你遇到与TensorFlow相关的问题，可以使用下面的替代代码，而不是本节中剩余的代码。 此替代方案基于预转换的PyTorch权重，使用与前一节描述的相同转换过程创建。有关详细信息，请参阅以下笔记本：\n",
    "[../02_alternative_weight_loading/weight-loading-pytorch.ipynb](../02_alternative_weight_loading/weight-loading-pytorch.ipynb) notebook.**\n",
    "\n",
    "```python\n",
    "file_name = \"gpt2-small-124M.pth\"\n",
    "# file_name = \"gpt2-medium-355M.pth\"\n",
    "# file_name = \"gpt2-large-774M.pth\"\n",
    "# file_name = \"gpt2-xl-1558M.pth\"\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device);\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- 首先，一些样板代码用于从OpenAI下载文件并将权重加载到Python中。\n",
    "- 由于OpenAI使用了 [TensorFlow](https://www.tensorflow.org/)，我们需要安装并使用TensorFlow来加载权重；[tqdm](https://github.com/tqdm/tqdm)是一个进度条库\n",
    "- 取消注释并运行下一个单元格以安装所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**\n",
    "\n",
    "- 在极少数情况下，上面的代码单元可能会导致`zsh: illegal hardware instruction python`错误，这可能是由于你机器上TensorFlow安装问题引起的。\n",
    "- 一位读者发现通过`conda`安装TensorFlow解决了这个问题，具体请参阅 [here](https://github.com/rasbt/LLMs-from-scratch/discussions/273#discussioncomment-12367888)\n",
    "- 你可以在这个辅助 [Python setup tutorial](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences#option-2-using-conda)中找到更多安装说明。\n",
    "\n",
    "---\n",
    "\n",
    "- 然后，我们可以按照以下方式下载1.24亿参数模型的权重：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- 另外，\"355M\"、\"774M\"和\"1558M\"也是支持的`model_size`参数。\n",
    "- 这些不同大小模型之间的差异总结在下图中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- 上面，我们将124M GPT-2模型权重加载到Python中，但仍然需要将其转移到我们的`GPTModel`实例中。\n",
    "- 首先，我们初始化一个新的`GPTModel`实例。\n",
    "- 请注意，原始的GPT模型在多头注意力模块中初始化查询、键和值矩阵的线性层时使用了偏置向量，这在我们的情况下不是必需或推荐的；然而，为了能够正确加载权重，我们还需要在实现中启用这些偏置向量，即将`qkv_bias`设置为`True`。\n",
    "- 我们还使用了原始GPT-2模型使用的`1024`个标记的上下文长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- 接下来的任务是将OpenAI的权重分配给我们`GPTModel`实例中对应的权重张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- 如果模型加载正确，我们可以使用之前的`generate`函数来生成新的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
   "metadata": {},
   "source": [
    "- 我们知道模型权重加载正确，因为模型能够生成连贯的文本；如果我们有任何小的错误，模型将无法做到这一点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
   "metadata": {},
   "source": [
    "- 对于从Hugging Face Hub加载权重的替代方法，请参阅 [../02_alternative_weight_loading](../02_alternative_weight_loading)。\n",
    "- 如果你对GPT架构与Llama架构（由Meta AI开发的一种流行的LLM）进行比较感兴趣，请参阅 [../07_gpt_to_llama](../07_gpt_to_llama) 中的附加内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
   "metadata": {},
   "source": [
    "## 摘要和要点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
   "metadata": {},
   "source": [
    "- 请参阅 [./gpt_train.py](./gpt_train.py) 脚本，这是一个独立的训练脚本。\n",
    "- The [./gpt_generate.py](./gpt_generate.py) 脚本从OpenAI加载预训练权重，并根据提示生成文本。\n",
    "- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb)中找到练习解答。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
