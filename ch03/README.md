# 第3章: 编写注意力机制

&nbsp;
## 主章节代码

- [01_main-chapter-code](01_main-chapter-code) 包含主章节代码

&nbsp;
## 附加材料

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) 实现并比较多头注意力的不同实现变体
- [03_understanding-buffers](03_understanding-buffers) 解释PyTorch缓冲区的概念，这些缓冲区用于实现第3章中的因果注意力机制

在下面的视频中，我提供了一个涵盖部分章节内容的代码演示作为补充材料：

<br>
<br>

[![视频链接](https://img.youtube.com/vi/-Ll8DtpNtvk/0.jpg)](https://www.youtube.com/watch?v=-Ll8DtpNtvk)
